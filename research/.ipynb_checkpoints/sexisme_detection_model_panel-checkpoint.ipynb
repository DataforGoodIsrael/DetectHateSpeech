{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pandas_summary import DataFrameSummary\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import multiprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn import model_selection, linear_model\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "DFS_PATH = '../data/detect_hate_speech_data.csv'\n",
    "\n",
    "SIZE = 100\n",
    "WINDOW = 10\n",
    "MIN_COUNT = 1\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20894\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DFS_PATH, sep='|', error_bad_lines=False)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20894"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>20894</td>\n",
       "      <td>20894</td>\n",
       "      <td>20894</td>\n",
       "      <td>20894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniques</th>\n",
       "      <td>20894</td>\n",
       "      <td>16844</td>\n",
       "      <td>16844</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_perc</th>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <td>numeric</td>\n",
       "      <td>categorical</td>\n",
       "      <td>categorical</td>\n",
       "      <td>categorical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Unnamed: 0     tweet_id         text        label\n",
       "counts            20894        20894        20894        20894\n",
       "uniques           20894        16844        16844            3\n",
       "missing               0            0            0            0\n",
       "missing_perc         0%           0%           0%           0%\n",
       "types           numeric  categorical  categorical  categorical"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = DataFrameSummary(df)\n",
    "dfs.columns_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['tweet_id', 'text', 'label']]\n",
    "df = df.drop_duplicates(subset=['tweet_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576424537065156608</td>\n",
       "      <td>@AAmbrosim a ta. Kkkk kkkk as feminazi</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570757526846881792</td>\n",
       "      <td>@Javanzord @Douglas_Feer feminazi</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>576536812283269121</td>\n",
       "      <td>bro my icon is literally feminazi misanandrist...</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>576428713023143936</td>\n",
       "      <td>RT @PhilGonc: @Independent \"Stop Male Violence...</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>572336406053322753</td>\n",
       "      <td>Sauce, capsicum and stock from a packet / jar....</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                               text  \\\n",
       "0  576424537065156608             @AAmbrosim a ta. Kkkk kkkk as feminazi   \n",
       "1  570757526846881792                  @Javanzord @Douglas_Feer feminazi   \n",
       "2  576536812283269121  bro my icon is literally feminazi misanandrist...   \n",
       "3  576428713023143936  RT @PhilGonc: @Independent \"Stop Male Violence...   \n",
       "4  572336406053322753  Sauce, capsicum and stock from a packet / jar....   \n",
       "\n",
       "    label  \n",
       "0  sexism  \n",
       "1  sexism  \n",
       "2  sexism  \n",
       "3  sexism  \n",
       "4  sexism  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16844"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax = df['label'].value_counts().plot(kind='bar', title=\"Label Distribution\")\n",
    "ax.set_xlabel(\"label\")\n",
    "ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['label']!='homophobia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label: It is a textual label, let's encode it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'none', 1: 'sexism'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_mappings = {index: label for index, label in \n",
    "                  enumerate(le.classes_)}\n",
    "labels_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16757"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    12866\n",
       "1     3891\n",
       "Name: new_label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['new_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7677985319567942"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12866/16757"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_vectors_mean(corpus, model, size):\n",
    "    result = []\n",
    "\n",
    "    for sent in corpus:\n",
    "        s = []\n",
    "        for w in sent:\n",
    "            try:\n",
    "                s.append(model[w])\n",
    "            except KeyError:\n",
    "                s.append(np.zeros(size))\n",
    "\n",
    "        result.append(np.mean(s, axis=0))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_w2v_corpus(df):\n",
    "    r = []\n",
    "    for i in range(len(df)):\n",
    "        r.append(gensim.utils.simple_preprocess(df[i]))\n",
    "    return r\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = re.sub(r'[,!@#$%^&*)(|/><\";:.?\\'\\\\}{]',\"\",text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], \n",
    "                                                    df['new_label'], \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['clean_text'] = [clean_text(text) for text in training['text'].values]\n",
    "testing['clean_text'] = [clean_text(text) for text in testing['text'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['text'] = read_w2v_corpus(training['text'])\n",
    "X_test = read_w2v_corpus(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44355827, 54616750)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(training['text'], \n",
    "                 size=SIZE, \n",
    "                 window=WINDOW,\n",
    "                 min_count=MIN_COUNT,\n",
    "                 workers=cores)\n",
    "\n",
    "model.train(training['text'], total_examples=model.corpus_count, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuel/anaconda/envs/venv_zencity_ai/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/samuel/anaconda/envs/venv_zencity_ai/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "training['w2v'] = transform_vectors_mean(training['text'], model, SIZE)\n",
    "X_test = transform_vectors_mean(X_test, model, SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for i in range(len(training)):\n",
    "    try:\n",
    "        if len(training['w2v'][i]) == 0:\n",
    "            print('youpi')\n",
    "    except:\n",
    "        t.append(i)\n",
    "        \n",
    "training = training.drop(t).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training['w2v']\n",
    "train_label = training['new_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15079"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.SGDClassifier(n_jobs=-1, max_iter=1000, tol=1e-4, n_iter=None)\n",
    "model_calibrated = CalibratedClassifierCV(base_estimator=clf, cv=3, method='sigmoid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=-1, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False),\n",
       "            cv=3, method='sigmoid')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_calibrated.fit(list(train), train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_calibrated.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8317422434367542"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89655172, 0.54952077])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, predictions, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84392265, 0.75438596])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, predictions, average=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1222,   56],\n",
       "       [ 226,  172]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification text TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_column(X):\n",
    "    return list(X['clean_text'].values)\n",
    "\n",
    "clf = linear_model.SGDClassifier(n_jobs=-1, max_iter=1000, tol=1e-4, n_iter=None)\n",
    "model_calibrated = CalibratedClassifierCV(base_estimator=clf, cv=3, method='sigmoid')\n",
    "\n",
    "tfidf_model = make_pipeline(FunctionTransformer(content_column, validate=False),\n",
    "                      TfidfVectorizer(min_df=0., max_df=1., use_idf=True, max_features=20000),\n",
    "                      model_calibrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('functiontransformer', FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "          func=<function content_column at 0x12c8af950>, inv_kw_args=None,\n",
       "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "          validate=False)), ('tfidfvectorizer', TfidfVectorizer(analy...\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False),\n",
       "            cv=3, method='sigmoid'))])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_model.fit(training, training['new_label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tfidf_model.predict(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8955847255369929"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93353589, 0.7566064 ])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, predictions, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90701107, 0.84735202])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, predictions, average=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1229,   49],\n",
       "       [ 126,  272]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification text LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_model = TruncatedSVD(n_components=500, \n",
    "                         algorithm='randomized',\n",
    "                         n_iter=10, \n",
    "                         random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.SGDClassifier(n_jobs=-1, max_iter=1000, tol=1e-4, n_iter=None)\n",
    "model_calibrated = CalibratedClassifierCV(base_estimator=clf, cv=3, method='sigmoid')\n",
    "\n",
    "lsa_model = make_pipeline(FunctionTransformer(content_column, validate=False),\n",
    "                      TfidfVectorizer(min_df=0., max_df=1., use_idf=True, max_features=20000),\n",
    "                      svd_model,\n",
    "                      model_calibrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('functiontransformer', FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "          func=<function content_column at 0x12c8af950>, inv_kw_args=None,\n",
       "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "          validate=False)), ('tfidfvectorizer', TfidfVectorizer(analy...\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False),\n",
       "            cv=3, method='sigmoid'))])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_model.fit(training, training['new_label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lsa_model.predict(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.863961813842482"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91524164, 0.65558912])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, predictions, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87181303, 0.8219697 ])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, predictions, average=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1231,   47],\n",
       "       [ 181,  217]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_feature(X):\n",
    "    return list(X['w2v'].values)\n",
    "\n",
    "def content_column(X):\n",
    "    return list(X['clean_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing['w2v'] = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.SGDClassifier(n_jobs=-1, max_iter=1000, tol=1e-4, n_iter=None)\n",
    "model_calibrated = CalibratedClassifierCV(base_estimator=clf, cv=3, method='sigmoid')\n",
    "\n",
    "pipe2 = make_pipeline(FunctionTransformer(w2v_feature, validate=False),\n",
    "                      RandomForestClassifier(n_estimators=500, n_jobs=-1))\n",
    "\n",
    "pipe3 = make_pipeline(FunctionTransformer(w2v_feature, validate=False),\n",
    "                      model_calibrated)\n",
    "\n",
    "pipe4 = make_pipeline(FunctionTransformer(w2v_feature, validate=False), \n",
    "                      GaussianNB())\n",
    "\n",
    "tfidf_model = make_pipeline(FunctionTransformer(content_column, validate=False),\n",
    "                      TfidfVectorizer(min_df=0., max_df=1., use_idf=True, max_features=20000),\n",
    "                      model_calibrated)\n",
    "\n",
    "lsa_model = make_pipeline(FunctionTransformer(content_column, validate=False),\n",
    "                      TfidfVectorizer(min_df=0., max_df=1., use_idf=True, max_features=20000),\n",
    "                      svd_model,\n",
    "                      model_calibrated)\n",
    "\n",
    "sclf = StackingClassifier(classifiers=[pipe2, pipe3, pipe4, lsa_model, tfidf_model],\n",
    "                          use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=RandomForestClassifier(n_estimators=50, n_jobs=-1, class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(average_probas=False,\n",
       "          classifiers=[Pipeline(memory=None,\n",
       "     steps=[('functiontransformer', FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "          func=<function w2v_feature at 0x12c8af2f0>, inv_kw_args=None,\n",
       "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "          validate=False)), ('ra...      validation_fraction=0.1, verbose=0, warm_start=False),\n",
       "            cv=3, method='sigmoid'))])],\n",
       "          meta_classifier=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=50, n_jobs=-1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "          store_train_meta_features=False, use_clones=True,\n",
       "          use_features_in_secondary=False, use_probas=True, verbose=0)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.fit(training, training['new_label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sclf.predict(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8406921241050119"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(testing['new_label'].values, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90433536, 0.52406417])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, predictions, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83410443, 0.90184049])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, predictions, average=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98748044, 0.36934673])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, predictions, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1262,   16],\n",
       "       [ 251,  147]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import preprocessing\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import (\n",
    "    Input, Dense, Flatten, LSTM, Conv1D, MaxPooling1D, GlobalMaxPool1D, \n",
    "    Embedding, Bidirectional, GlobalMaxPooling1D, Dropout\n",
    ")\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16757\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DFS_PATH, sep='|', error_bad_lines=False)\n",
    "df = df.drop_duplicates(subset=['tweet_id']).reset_index(drop=True)\n",
    "df = df[df['label']!='homophobia']\n",
    "labels = le.fit_transform(df['label'])\n",
    "df['new_label'] = labels\n",
    "print(len(df))\n",
    "df = df[['text','new_label']].dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_binary = to_categorical(df['new_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26828 unique tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['text',      0,      1,      2,      3,      4,      5,      6,      7,\n",
       "            8,\n",
       "       ...\n",
       "          118,    119,    120,    121,    122,    123,    124,    125,    126,\n",
       "          127],\n",
       "      dtype='object', length=129)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_len=128\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "x = tokenizer.texts_to_sequences(df['text'])\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "padded = pad_sequences(x, maxlen=max_len) \n",
    "\n",
    "concat_df = pd.concat([df['text'], pd.DataFrame(padded)],axis=1)\n",
    "concat_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(concat_df, \n",
    "                                                    y_binary, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, \n",
    "                                                y_test, \n",
    "                                                test_size=0.5, \n",
    "                                                random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_size = X_train.shape[1]\n",
    "\n",
    "input_shape = (sentence_size,)\n",
    "sentence_indices = Input(shape=input_shape)\n",
    "\n",
    "max_features = 1000  # max vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['text'], axis = 1).values\n",
    "X_val = X_val.drop(['text'], axis = 1).values\n",
    "X_test = X_test.drop(['text'], axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 100)               2000100   \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 2,005,252\n",
      "Trainable params: 2,005,252\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(max_features, 128, trainable=True)\n",
    "X = embedding_layer(sentence_indices)\n",
    "d1 = Dense(100)(sequence_input)\n",
    "d2 = Dense(50)(d1)\n",
    "preds = Dense(y_binary.shape[1], activation='softmax')(d2)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model_summary = model.summary()\n",
    "print(model_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary = []\n",
    "model.summary(print_fn=lambda x: model_summary.append(x + '\\n'))\n",
    "model_summary = ' '.join(model_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"nn.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15081 samples, validate on 838 samples\n",
      "Epoch 1/20\n",
      "15081/15081 [==============================] - 5s 332us/step - loss: 0.5762 - acc: 0.7509 - val_loss: 0.5053 - val_acc: 0.7554\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.75537, saving model to nn.best.hdf5\n",
      "Epoch 2/20\n",
      "15081/15081 [==============================] - 6s 384us/step - loss: 0.4075 - acc: 0.7858 - val_loss: 0.3977 - val_acc: 0.8162\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.75537 to 0.81623, saving model to nn.best.hdf5\n",
      "Epoch 3/20\n",
      "15081/15081 [==============================] - 5s 327us/step - loss: 0.2557 - acc: 0.9107 - val_loss: 0.3188 - val_acc: 0.8687\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.81623 to 0.86874, saving model to nn.best.hdf5\n",
      "Epoch 4/20\n",
      "15081/15081 [==============================] - 4s 275us/step - loss: 0.1542 - acc: 0.9454 - val_loss: 0.3174 - val_acc: 0.8723\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.86874 to 0.87232, saving model to nn.best.hdf5\n",
      "Epoch 5/20\n",
      "15081/15081 [==============================] - 4s 274us/step - loss: 0.1013 - acc: 0.9661 - val_loss: 0.3306 - val_acc: 0.8747\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.87232 to 0.87470, saving model to nn.best.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x12c315978>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(norm_corpus_matrix_train,\n",
    "          y_train,\n",
    "          batch_size=1024,\n",
    "          epochs=20,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=(norm_corpus_matrix_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "838/838 [==============================] - 0s 132us/step\n",
      "Test loss: 0.3190672993659973\n",
      "Test accuracy: 0.8926014304161072\n"
     ]
    }
   ],
   "source": [
    "#load best model\n",
    "model.load_weights(filepath)\n",
    "\n",
    "score = model.evaluate(norm_corpus_matrix_test, y_test, batch_size=1024)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers Based Algorithme"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/2004.06465.pdf\n",
    "\n",
    "@article{aluru2020deep,\n",
    "  title={Deep Learning Models for Multilingual Hate Speech Detection},\n",
    "  author={Aluru, Sai Saket and Mathew, Binny and Saha, Punyajoy and Mukherjee, Animesh},\n",
    "  journal={arXiv preprint arXiv:2004.06465},\n",
    "  year={2020}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_hf(text, model):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text, max_length=512)).unsqueeze(0)  # Batch size 1\n",
    "    # 512 max lenght for certain model\n",
    "    outputs = model(input_ids)\n",
    "    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
    "    sentence_embedding = torch.mean(last_hidden_states[0], dim=0)\n",
    "    \n",
    "    return sentence_embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@AAmbrosim a ta. Kkkk kkkk as feminazi'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "2380.1703238487244\n"
     ]
    }
   ],
   "source": [
    "dehatebert_emb = []\n",
    "start = time.time()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if i%1000 == 0:\n",
    "        print(i)\n",
    "    dehatebert_emb.append(get_embeddings_hf(df['text'][i], model))\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dehatebert_emb, \n",
    "                                                    df['new_label'], \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=-1, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False),\n",
       "            cv=3, method='sigmoid')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = linear_model.SGDClassifier(n_jobs=-1, max_iter=1000, tol=1e-4, n_iter=None)\n",
    "model_calibrated = CalibratedClassifierCV(base_estimator=clf, cv=3, method='sigmoid')\n",
    "model_calibrated.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8424821002386634"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model_calibrated.predict(X_test)\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90185874, 0.60120846])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, predictions, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85906516, 0.75378788])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, predictions, average=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94913928, 0.5       ])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, predictions, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1213,   65],\n",
       "       [ 199,  199]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Zencity_env",
   "language": "python",
   "name": "zencity_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
